// Handler for text generation requests from the Ollama API.
//
// This module provides security-enhanced handlers for text generation
// requests, scanning both prompts and responses for policy violations.
use axum::{extract::State, response::Response, Json};
use tracing::{debug, error, info};

use crate::handlers::utils::{
    build_json_response, build_violation_response, format_security_violation_message,
    handle_streaming_request, log_security_failure,
};
use crate::handlers::ApiError;
use crate::stream::SecurityAssessable;
use crate::types::{GenerateRequest, GenerateResponse};
use crate::AppState;

// Implementation of SecurityAssessable for GenerateResponse to enable
// security scanning of responses.
impl SecurityAssessable for crate::types::GenerateResponse {
    fn get_content_for_assessment(&self) -> Option<(&str, &str)> {
        Some((&self.response, "generate_response"))
    }
}

// Handles text generation requests with security assessment.
//
// This handler:
// 1. Performs security checks on the input prompt
// 2. Routes the request to Ollama if the prompt passes security checks
// 3. Scans the generated response for security issues before returning to client
// 4. Handles both streaming and non-streaming requests
//
// # Arguments
//
// * `State(state)` - Application state containing client connections
// * `Json(request)` - The generation request from the client
//
// # Returns
//
// * `Ok(Response)` - The generation response
// * `Err(ApiError)` - If an error occurs during processing
pub async fn handle_generate(
    State(state): State<AppState>,
    Json(mut request): Json<GenerateRequest>,
) -> Result<Response, ApiError> {
    // Ensure stream parameter is explicitly set
    request.stream = Some(false);

    debug!("Received generate request for model: {}", request.model);

    // Check the input prompt for security violations
    if let Err(response) = assess_generate_prompt(&state, &request).await? {
        return Ok(response);
    }

    // Route based on streaming or non-streaming mode
    if request.stream.unwrap() {
        debug!("Handling streaming generate request");
        handle_streaming_generate(State(state), Json(request)).await
    } else {
        debug!("Handling non-streaming generate request");
        handle_non_streaming_generate(State(state), Json(request)).await
    }
}

// Assesses a generation prompt for security policy violations.
//
// # Arguments
//
// * `state` - Application state containing security client
// * `request` - The generation request containing the prompt to assess
//
// # Returns
//
// * `Ok(Ok(()))` - If the prompt passes security checks
// * `Ok(Err(Response))` - If security violation is detected, with appropriate response
// * `Err(ApiError)` - If an error occurs during security assessment
async fn assess_generate_prompt(
    state: &AppState,
    request: &GenerateRequest,
) -> Result<Result<(), Response>, ApiError> {
    // Check input prompt
    let assessment = state
        .security_client
        .assess_content(&request.prompt, &request.model, true)
        .await?;

    // If the content is not safe, create a blocked response
    if !assessment.is_safe {
        log_security_failure("prompt", &assessment.category, &assessment.action);
        info!("Blocked generation request due to security violation in prompt");

        let blocked_message =
            format_security_violation_message(&assessment.category, &assessment.action);

        let response = GenerateResponse {
            model: request.model.clone(),
            created_at: chrono::Utc::now().to_rfc3339(),
            response: blocked_message,
            context: None,
            done: true,
        };

        return Ok(Err(build_violation_response(response)?));
    }

    info!("Prompt passed security assessment");
    Ok(Ok(()))
}

// Handles non-streaming generate requests.
//
// # Arguments
//
// * `State(state)` - Application state containing client connections
// * `Json(request)` - The generation request from the client
//
// # Returns
//
// * `Ok(Response)` - The generation response
// * `Err(ApiError)` - If an error occurs during processing
async fn handle_non_streaming_generate(
    State(state): State<AppState>,
    Json(request): Json<GenerateRequest>,
) -> Result<Response, ApiError> {
    debug!("Processing non-streaming generate request");

    // Forward request to Ollama
    let response = state
        .ollama_client
        .forward("/api/generate", &request)
        .await?;

    // Read response body
    let body_bytes = response.bytes().await.map_err(|e| {
        error!("Failed to read response body: {}", e);
        ApiError::InternalError("Failed to read response body".to_string())
    })?;

    // Parse response
    let mut response_body: GenerateResponse = serde_json::from_slice(&body_bytes).map_err(|e| {
        error!("Failed to parse response: {}", e);
        ApiError::InternalError("Failed to parse response".to_string())
    })?;

    // Check model output for security issues
    let assessment = state
        .security_client
        .assess_content(&response_body.response, &request.model, false)
        .await?;

    // If response is not safe, replace content with security message
    if !assessment.is_safe {
        log_security_failure("response", &assessment.category, &assessment.action);
        info!("Blocked unsafe AI-generated content");

        // Replace the content with security message
        response_body.response =
            format_security_violation_message(&assessment.category, &assessment.action);

        return build_violation_response(response_body);
    }

    // Return safe response
    Ok(build_json_response(body_bytes)?)
}

// Handles streaming generate requests.
//
// # Arguments
//
// * `State(state)` - Application state containing client connections
// * `Json(request)` - The generation request from the client
//
// # Returns
//
// * `Ok(Response)` - The streaming response
// * `Err(ApiError)` - If an error occurs during processing
async fn handle_streaming_generate(
    State(state): State<AppState>,
    Json(request): Json<GenerateRequest>,
) -> Result<Response, ApiError> {
    debug!("Setting up streaming generate request");

    let model = request.model.clone();
    handle_streaming_request::<GenerateRequest, GenerateResponse>(
        &state,
        request,
        "/api/generate",
        &model,
    )
    .await
}
